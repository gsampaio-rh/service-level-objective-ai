{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLO Definition and Optimization Using AI\n",
    "\n",
    "The purpose of this notebook is to guide operations professionals in experimenting with using AI to define Service Level Objectives (SLOs). By leveraging machine learning techniques, we aim to optimize the definition of SLOs to better align with business objectives and improve overall service quality.\n",
    "\n",
    "This notebook is designed for operations professionals who have a basic understanding of data analysis and machine learning. It aims to provide a step-by-step guide that is easy to follow and well-documented.\n",
    "\n",
    "## Importance of AI in Defining SLOs\n",
    "Service Level Objectives (SLOs) are critical in ensuring that a service meets the expected performance and reliability standards. Traditionally, SLOs are defined based on historical data and expert judgment. However, with the advent of AI and machine learning, we can leverage data-driven approaches to optimize these definitions, leading to more accurate and effective SLOs.\n",
    "\n",
    "## Overview of the Notebook\n",
    "This notebook is structured as follows:\n",
    "1. **Generating Mock Data:** Creating realistic mock data for experimentation.\n",
    "2. **Exploratory Data Analysis (EDA):** Visualizing and understanding the data.\n",
    "3. **Defining the SLO without AI:** Explaining the traditional method of defining SLOs.\n",
    "4. **Feature Engineering:** Creating and selecting features for SLO definitions.\n",
    "5. **Model Selection and Training:** Using machine learning models for SLO optimization.\n",
    "6. **Model Evaluation:** Evaluating the performance of the models.\n",
    "7. **Define Optimal SLOs:** Presenting and justifying the final SLO definitions.\n",
    "8. **Show KPIs and Comparisons:** Demonstrating the effectiveness of the optimized SLOs.\n",
    "9. **Conclusion:** Summarizing the findings and suggesting areas for further improvement.\n",
    "\n",
    "Let's dive into each section step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Required Libraries\n",
    "%pip install pandas numpy seaborn matplotlib scikit-learn tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all library imports at the beginning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Mock Data\n",
    "\n",
    "In this section, we generate realistic mock data to simulate real-world scenarios. This data will be used for experimentation and to demonstrate the process of defining and optimizing Service Level Objectives (SLOs) using AI.\n",
    "\n",
    "### Purpose\n",
    "Generating realistic mock data is crucial for testing and validating our machine learning models. It helps us to:\n",
    "- Simulate real-world conditions.\n",
    "- Validate the effectiveness of our models.\n",
    "- Ensure the robustness and reliability of our approach.\n",
    "\n",
    "### Implementation\n",
    "We will create a dataset that includes various metrics relevant to SLOs, such as response times, error rates, and throughput. This dataset will be used throughout the notebook for exploratory data analysis, feature engineering, model training, and evaluation.\n",
    "\n",
    "### Displaying the Generated Data\n",
    "After generating the data, we will display the first few rows to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate mock data\n",
    "def generate_mock_data(seed=42, data_size=1000):\n",
    "    \"\"\"\n",
    "    Generate mock data for SLO experimentation.\n",
    "\n",
    "    Parameters:\n",
    "    seed (int): Random seed for reproducibility.\n",
    "    data_size (int): Number of data points to generate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing generated mock data.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    timestamps = pd.date_range(start=\"2023-01-01\", periods=data_size, freq=\"H\")\n",
    "\n",
    "    latency = np.random.normal(loc=100, scale=20, size=data_size)\n",
    "    error_rate = np.random.normal(loc=0.05, scale=0.01, size=data_size)\n",
    "    error_rate = np.clip(error_rate, 0, 1)\n",
    "    throughput = np.random.normal(loc=200, scale=50, size=data_size)\n",
    "    availability = np.random.choice([0, 1], size=data_size, p=[0.01, 0.99])\n",
    "\n",
    "    anomaly_fraction = 0.05\n",
    "    anomaly_indices = np.random.choice(\n",
    "        data_size, size=int(anomaly_fraction * data_size), replace=False\n",
    "    )\n",
    "\n",
    "    latency[anomaly_indices] = np.random.normal(\n",
    "        loc=300, scale=50, size=len(anomaly_indices)\n",
    "    )\n",
    "    error_rate[anomaly_indices] = np.random.normal(\n",
    "        loc=0.2, scale=0.05, size=len(anomaly_indices)\n",
    "    )\n",
    "    throughput[anomaly_indices] = np.random.normal(\n",
    "        loc=100, scale=20, size=len(anomaly_indices)\n",
    "    )\n",
    "    availability[anomaly_indices] = np.random.choice(\n",
    "        [0, 1], size=len(anomaly_indices), p=[0.2, 0.8]\n",
    "    )\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"timestamp\": timestamps,\n",
    "            \"latency\": latency,\n",
    "            \"error_rate\": error_rate,\n",
    "            \"throughput\": throughput,\n",
    "            \"availability\": availability,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data and display the first few rows\n",
    "data = generate_mock_data()\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(tabulate(data.head(), headers=\"keys\", tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "In this section, we perform Exploratory Data Analysis (EDA) to gain insights into the generated mock data. EDA is crucial for understanding the dataset, identifying patterns, and preparing the data for machine learning models.\n",
    "\n",
    "### Overview of the Dataset\n",
    "We will start by getting a summary of the dataset, including data types and basic statistics.\n",
    "\n",
    "### Calculating Descriptive Statistics\n",
    "We will calculate descriptive statistics to understand the central tendency, dispersion, and shape of the data distributions.\n",
    "\n",
    "### Visualizing Data Distributions and Relationships\n",
    "We will create visualizations to identify trends, patterns, and correlations between variables. This helps in understanding the data better and uncovering any underlying structure.\n",
    "\n",
    "### Detecting Outliers\n",
    "We will identify and handle outliers in the data to ensure it does not adversely affect our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate descriptive statistics\n",
    "def calculate_statistics(data, columns):\n",
    "    \"\"\"\n",
    "    Calculate basic descriptive statistics for multiple columns in the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    columns (list): List of column names for which to calculate statistics.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the calculated statistics for each column.\n",
    "    \"\"\"\n",
    "    stats = data[columns].describe().transpose()\n",
    "    stats.reset_index(inplace=True)\n",
    "    stats.rename(columns={\"index\": \"Metric\"}, inplace=True)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize statistics in a matrix format\n",
    "def visualize_statistics_matrix(stats):\n",
    "    \"\"\"\n",
    "    Visualize the basic descriptive statistics in a matrix format.\n",
    "\n",
    "    Parameters:\n",
    "    stats (pd.DataFrame): The dataframe containing the statistics to visualize.\n",
    "    \"\"\"\n",
    "    print(tabulate(stats, headers=\"keys\", tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics over time in a 2x2 grid\n",
    "def plot_metrics_over_time(data, x_col, y_cols, titles, colors):\n",
    "    \"\"\"\n",
    "    Plot multiple metrics over time in a 2x2 grid layout.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    x_col (str): The column name for the x-axis (timestamp).\n",
    "    y_cols (list): List of column names for the y-axis (metrics).\n",
    "    titles (list): List of titles for each subplot.\n",
    "    colors (list): List of colors for each metric plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    for ax, y_col, title, color in zip(axes, y_cols, titles, colors):\n",
    "        sns.lineplot(data=data, x=x_col, y=y_col, color=color, ax=ax, linewidth=1.5)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "        ax.set_ylabel(y_col.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot histograms in a 2x2 grid\n",
    "def plot_histograms(data, columns, colors):\n",
    "    \"\"\"\n",
    "    Plot histograms for multiple columns in a 2x2 grid layout.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    columns (list): List of column names for which to plot histograms.\n",
    "    colors (list): List of colors for each histogram.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "    for ax, column, color in zip(axes, columns, colors):\n",
    "        sns.histplot(data[column], bins=30, kde=True, ax=ax, color=color)\n",
    "        ax.set_title(f\"Distribution of {column.capitalize()}\")\n",
    "        ax.set_xlabel(column.capitalize())\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot KDE plots in a 2x2 grid\n",
    "def plot_kde(data, columns, titles, colors):\n",
    "    \"\"\"\n",
    "    Plot KDE plots for multiple columns in a 2x2 grid layout.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    columns (list): List of column names for the data to be plotted.\n",
    "    titles (list): List of titles for each subplot.\n",
    "    colors (list): List of colors for each KDE plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    for ax, column, title, color in zip(axes, columns, titles, colors):\n",
    "        sns.kdeplot(data[column], fill=True, color=color, ax=ax)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(column.capitalize())\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot boxplots in a 2x2 grid\n",
    "def plot_boxplots(data, columns, colors):\n",
    "    \"\"\"\n",
    "    Plot boxplots for multiple columns in a 2x2 grid layout to identify outliers.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    columns (list): List of column names for which to plot boxplots.\n",
    "    colors (list): List of colors for each boxplot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "    for ax, column, color in zip(axes, columns, colors):\n",
    "        sns.boxplot(x=data[column], ax=ax, color=color)\n",
    "        ax.set_title(f\"Boxplot of {column.capitalize()}\")\n",
    "        ax.set_xlabel(column.capitalize())\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA: Calculate and visualize statistics, plot metrics, histograms, KDE plots, and boxplots\n",
    "\n",
    "# Columns to analyze\n",
    "columns_to_analyze = [\"latency\", \"error_rate\", \"throughput\", \"availability\"]\n",
    "\n",
    "# Calculate statistics for the specified columns\n",
    "stats = calculate_statistics(data, columns_to_analyze)\n",
    "\n",
    "# Visualize the statistics in a matrix format\n",
    "visualize_statistics_matrix(stats)\n",
    "\n",
    "# Define titles and colors for plots\n",
    "titles = [\n",
    "    \"Latency Over Time\",\n",
    "    \"Error Rate Over Time\",\n",
    "    \"Throughput Over Time\",\n",
    "    \"Availability Over Time\",\n",
    "]\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "\n",
    "# Plot metrics over time\n",
    "plot_metrics_over_time(data, \"timestamp\", columns_to_analyze, titles, colors)\n",
    "\n",
    "# Plot histograms\n",
    "plot_histograms(data, columns_to_analyze, colors)\n",
    "\n",
    "# Plot KDE plots\n",
    "# plot_kde(data, columns_to_analyze, titles, colors)\n",
    "\n",
    "# Plot boxplots\n",
    "# plot_boxplots(data, columns_to_analyze, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Defining Service Level Objectives (SLOs) Without AI\n",
    "\n",
    "In this section, we will define Service Level Objectives (SLOs) based on traditional methods. This involves understanding the business context, identifying relevant metrics, and making informed assumptions.\n",
    "\n",
    "### Understanding Business Context and Goals\n",
    "We need to comprehend the importance of SLOs in the business and how they align with organizational goals. This includes:\n",
    "- Providing an overview of the service or system being monitored.\n",
    "- Describing the business goals that the SLOs should support, such as ensuring high availability and minimizing latency.\n",
    "- Identifying key stakeholders and their requirements.\n",
    "\n",
    "### Identifying Relevant Metrics\n",
    "We will determine which metrics are critical for monitoring and maintaining service levels. These typically include:\n",
    "- **Latency:** Define acceptable latency thresholds to ensure a responsive user experience.\n",
    "- **Error Rate:** Establish acceptable error rates to maintain service reliability.\n",
    "- **Throughput:** Set throughput targets to ensure the system can handle the expected load.\n",
    "- **Availability:** Define availability targets to ensure the system is operational when needed.\n",
    "\n",
    "### Making Assumptions Based on Historical Data and Industry Standards\n",
    "To define realistic and achievable SLOs, we will make assumptions based on:\n",
    "- **Historical Data Analysis:** Using historical data to inform the SLO thresholds.\n",
    "- **Industry Standards:** Considering industry standards and best practices for defining SLOs.\n",
    "- **Stakeholder Input:** Incorporating input from stakeholders to ensure the SLOs align with their expectations.\n",
    "\n",
    "### Calculating and Evaluating Traditional SLOs\n",
    "We will calculate the SLO thresholds based on the collected data and evaluate the data against these thresholds to generate results. This involves:\n",
    "- Calculating the 95th percentile for latency, error rate, and availability to set realistic upper thresholds.\n",
    "- Calculating the 5th percentile for throughput to set a realistic lower threshold.\n",
    "- Implementing a function to evaluate the data against these thresholds and generate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate SLO thresholds\n",
    "def calculate_slo_thresholds(data):\n",
    "    \"\"\"\n",
    "    Calculate SLO thresholds based on the collected data.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with SLO thresholds for latency, error rate, throughput, and availability.\n",
    "    \"\"\"\n",
    "    slo_thresholds = {\n",
    "        \"latency\": np.percentile(data[\"latency\"], 95),  # 95th percentile for latency\n",
    "        \"error_rate\": np.percentile(\n",
    "            data[\"error_rate\"], 95\n",
    "        ),  # 95th percentile for error rate\n",
    "        \"throughput\": np.percentile(\n",
    "            data[\"throughput\"], 5\n",
    "        ),  # 5th percentile for throughput\n",
    "        \"availability\": np.percentile(\n",
    "            data[\"availability\"] * 100, 95\n",
    "        ),  # 95th percentile for availability\n",
    "    }\n",
    "    return slo_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate SLOs\n",
    "def evaluate_slos(data, slo_thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate the data against the defined SLO thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    slo_thresholds (dict): A dictionary with SLO thresholds.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the evaluation results for each metric.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results[\"latency\"] = (data[\"latency\"] < slo_thresholds[\"latency\"]).mean() * 100\n",
    "    results[\"error_rate\"] = (\n",
    "        data[\"error_rate\"] < slo_thresholds[\"error_rate\"]\n",
    "    ).mean() * 100\n",
    "    results[\"throughput\"] = (\n",
    "        data[\"throughput\"] >= slo_thresholds[\"throughput\"]\n",
    "    ).mean() * 100\n",
    "    results[\"availability\"] = (\n",
    "        data[\"availability\"] * 100 >= slo_thresholds[\"availability\"]\n",
    "    ).mean() * 100\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SLO thresholds and evaluate SLOs, display the results\n",
    "\n",
    "# Calculate SLO thresholds\n",
    "slo_thresholds = calculate_slo_thresholds(data)\n",
    "\n",
    "# Evaluate SLOs\n",
    "slo_evaluation_results = evaluate_slos(data, slo_thresholds)\n",
    "\n",
    "# Display SLO thresholds and evaluation results\n",
    "print(\"Defined SLO Thresholds:\")\n",
    "for metric, threshold in slo_thresholds.items():\n",
    "    print(f\"{metric.capitalize()} SLO Threshold: {threshold:.2f}\")\n",
    "\n",
    "print(\"\\nSLO Evaluation Results:\")\n",
    "for metric, result in slo_evaluation_results.items():\n",
    "    print(f\"{metric.capitalize()} SLO: {result:.2f}% of requests meet the SLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bubble_charts_with_slos(data, columns, titles, slo_thresholds, colors):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, column, title, color in zip(axes, columns, titles, colors):\n",
    "        value_counts = data[column].value_counts().reset_index()\n",
    "        value_counts.columns = [column, \"Frequency\"]\n",
    "\n",
    "        ax.scatter(\n",
    "            value_counts.index,\n",
    "            value_counts[column],\n",
    "            s=value_counts[\"Frequency\"] * 20,  # Adjust size for better visualization\n",
    "            color=color,\n",
    "            edgecolor=\"black\",\n",
    "            alpha=0.6,\n",
    "        )\n",
    "\n",
    "        ax.set_title(title, fontsize=14, weight=\"bold\")\n",
    "        ax.set_xlabel(\"Index\", fontsize=12)\n",
    "        ax.set_ylabel(column.capitalize(), fontsize=12)\n",
    "\n",
    "        # Plot SLO threshold line\n",
    "        slo_threshold = slo_thresholds[column]\n",
    "        ax.axhline(\n",
    "            y=slo_threshold,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"SLO Threshold: {slo_threshold:.2f}\",\n",
    "        )\n",
    "        ax.legend(loc=\"upper right\", fontsize=12)\n",
    "\n",
    "        ax.grid(\n",
    "            visible=True, which=\"major\", color=\"lightgrey\", linestyle=\"-\", linewidth=0.5\n",
    "        )\n",
    "        ax.grid(\n",
    "            visible=True,\n",
    "            which=\"minor\",\n",
    "            color=\"lightgrey\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot histograms with SLO thresholds and annotations\n",
    "def plot_histograms_with_slo(data, slo_thresholds, columns, colors):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, column, color in zip(axes, columns, colors):\n",
    "        sns.histplot(data[column], bins=30, kde=True, ax=ax, color=color)\n",
    "        ax.axvline(\n",
    "            x=slo_thresholds[column],\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"SLO Threshold: {slo_thresholds[column]:.2f}\",\n",
    "        )\n",
    "        ax.set_title(f\"Distribution of {column.capitalize()} with SLO\", fontsize=14)\n",
    "        ax.set_xlabel(column.capitalize(), fontsize=12)\n",
    "        ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "        ax.legend(loc=\"upper right\", fontsize=12)\n",
    "\n",
    "        # Add annotations\n",
    "        if column in [\"latency\", \"error_rate\", \"availability\"]:\n",
    "            ax.annotate(\n",
    "                \"Values below threshold\\nare acceptable\",\n",
    "                xy=(slo_thresholds[column], ax.get_ylim()[1] / 2),\n",
    "                xytext=(\n",
    "                    slo_thresholds[column] + ax.get_xlim()[1] / 10,\n",
    "                    ax.get_ylim()[1] / 1.5,\n",
    "                ),\n",
    "                arrowprops=dict(facecolor=\"black\", shrink=0.05),\n",
    "                fontsize=12,\n",
    "                color=\"black\",\n",
    "            )\n",
    "        else:\n",
    "            ax.annotate(\n",
    "                \"Values above threshold\\nare acceptable\",\n",
    "                xy=(slo_thresholds[column], ax.get_ylim()[1] / 2),\n",
    "                xytext=(\n",
    "                    slo_thresholds[column] - ax.get_xlim()[1] / 10,\n",
    "                    ax.get_ylim()[1] / 1.5,\n",
    "                ),\n",
    "                arrowprops=dict(facecolor=\"black\", shrink=0.05),\n",
    "                fontsize=12,\n",
    "                color=\"black\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all histograms in a 2x2 grid layout with SLO thresholds\n",
    "plot_histograms_with_slo(data, slo_thresholds, columns_to_analyze, colors)\n",
    "\n",
    "\n",
    "# Plot all bubble charts in a 2x2 grid layout with SLO thresholds\n",
    "plot_bubble_charts_with_slos(data, columns_to_analyze, titles, slo_thresholds, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this section, we will focus on creating and transforming features to enhance the predictive power of our machine learning models. Feature engineering is a critical step in the machine learning pipeline that can significantly impact model performance.\n",
    "\n",
    "### Creating and Transforming Features\n",
    "We will develop new features based on domain knowledge and data exploration insights. Additionally, we will apply necessary transformations to improve feature representation.\n",
    "\n",
    "### Handling Missing Values\n",
    "We will ensure that missing values are appropriately handled to prevent issues in model training and evaluation.\n",
    "\n",
    "### Normalizing and Standardizing Features\n",
    "To ensure consistency and improve model performance, we will normalize or standardize features to bring them onto a comparable scale.\n",
    "\n",
    "### Displaying Transformed Data\n",
    "We will display the first few rows of the transformed data to understand the changes and improvements made during feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform feature engineering\n",
    "def feature_engineering(data):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the original data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe with new and transformed features.\n",
    "    \"\"\"\n",
    "    # Handle invalid values for log transformation\n",
    "    data[\"latency\"] = data[\"latency\"].apply(lambda x: x if x > 0 else 1)\n",
    "    data[\"error_rate\"] = data[\"error_rate\"].apply(lambda x: x if x > 0 else 1e-6)\n",
    "    data[\"throughput\"] = data[\"throughput\"].apply(lambda x: x if x > 0 else 1)\n",
    "\n",
    "    # Create new features based on domain knowledge\n",
    "    data[\"latency_log\"] = np.log1p(\n",
    "        data[\"latency\"]\n",
    "    )  # Log transformation to handle skewness\n",
    "    data[\"error_rate_log\"] = np.log1p(\n",
    "        data[\"error_rate\"]\n",
    "    )  # Log transformation for error rate\n",
    "    data[\"throughput_log\"] = np.log1p(\n",
    "        data[\"throughput\"]\n",
    "    )  # Log transformation for throughput\n",
    "\n",
    "    # Example feature: Rolling mean and standard deviation\n",
    "    data[\"latency_roll_mean\"] = data[\"latency\"].rolling(window=24, min_periods=1).mean()\n",
    "    data[\"latency_roll_std\"] = data[\"latency\"].rolling(window=24, min_periods=1).std()\n",
    "\n",
    "    # Extract time-based features\n",
    "    data[\"hour\"] = data[\"timestamp\"].dt.hour\n",
    "    data[\"day_of_week\"] = data[\"timestamp\"].dt.dayofweek\n",
    "    data[\"is_weekend\"] = data[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    # Handle missing values by filling with the mean of the column\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering and display the first few rows of transformed data\n",
    "\n",
    "# Apply feature engineering\n",
    "data_fe = feature_engineering(data)\n",
    "\n",
    "# Display the first few rows of the transformed data\n",
    "print(tabulate(data_fe.head(), headers=\"keys\", tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection and Visualization\n",
    "\n",
    "In this section, we will identify anomalies in the metrics and visualize them. Anomalies are defined as points where the metric values deviate significantly from the norm.\n",
    "\n",
    "### Identifying Anomalies Based on Statistical Thresholds\n",
    "We will define anomalies using statistical thresholds, such as the 95th percentile for metrics like latency and error rate, and the 5th percentile for throughput. This helps us pinpoint unusual behavior in the data.\n",
    "\n",
    "### Visualizing Anomalies Over Time\n",
    "We will create plots to visualize the anomalies over time, making it easier to understand when and how often these anomalies occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify anomalies\n",
    "def identify_anomalies(data):\n",
    "    \"\"\"\n",
    "    Identify anomalies in the dataset based on statistical thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the metrics.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with additional columns indicating anomalies.\n",
    "    \"\"\"\n",
    "    anomaly_thresholds = {\n",
    "        \"latency\": np.percentile(data[\"latency\"], 95),\n",
    "        \"error_rate\": np.percentile(data[\"error_rate\"], 95),\n",
    "        \"throughput\": np.percentile(data[\"throughput\"], 5),\n",
    "        \"availability\": np.percentile(data[\"availability\"] * 100, 95),\n",
    "    }\n",
    "\n",
    "    data[\"latency_anomaly\"] = data[\"latency\"] > anomaly_thresholds[\"latency\"]\n",
    "    data[\"error_rate_anomaly\"] = data[\"error_rate\"] > anomaly_thresholds[\"error_rate\"]\n",
    "    data[\"throughput_anomaly\"] = data[\"throughput\"] < anomaly_thresholds[\"throughput\"]\n",
    "    data[\"availability_anomaly\"] = (data[\"availability\"] * 100) < anomaly_thresholds[\n",
    "        \"availability\"\n",
    "    ]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify anomalies and display the first few rows of the data with anomalies\n",
    "\n",
    "# Identify anomalies\n",
    "data_with_anomalies = identify_anomalies(data_fe)\n",
    "\n",
    "# Display the first few rows of the data with anomalies\n",
    "print(tabulate(data_with_anomalies.head(), headers=\"keys\", tablefmt=\"psql\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot anomalies in a 2x2 grid layout\n",
    "def plot_anomalies(data, columns, anomaly_columns, titles, colors, anomaly_color=\"red\"):\n",
    "    \"\"\"\n",
    "    Plot anomalies in a 2x2 grid layout.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data and anomalies.\n",
    "    columns (list): List of column names for the data to be plotted.\n",
    "    anomaly_columns (list): List of column names indicating anomalies.\n",
    "    titles (list): List of titles for each subplot.\n",
    "    colors (list): List of colors for each plot.\n",
    "    anomaly_color (str): Color to use for highlighting anomalies.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, column, anomaly_column, title, color in zip(\n",
    "        axes, columns, anomaly_columns, titles, colors\n",
    "    ):\n",
    "        ax.plot(\n",
    "            data[\"timestamp\"],\n",
    "            data[column],\n",
    "            color=color,\n",
    "            label=f\"Normal {column.capitalize()}\",\n",
    "        )\n",
    "        ax.scatter(\n",
    "            data.loc[data[anomaly_column], \"timestamp\"],\n",
    "            data.loc[data[anomaly_column], column],\n",
    "            color=anomaly_color,\n",
    "            label=\"Anomalies\",\n",
    "        )\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Timestamp\")\n",
    "        ax.set_ylabel(column.capitalize())\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomalies in a 2x2 grid layout\n",
    "\n",
    "# Define columns and titles for the plots\n",
    "columns_to_plot = [\"latency\", \"error_rate\", \"throughput\", \"availability\"]\n",
    "anomaly_columns = [\n",
    "    \"latency_anomaly\",\n",
    "    \"error_rate_anomaly\",\n",
    "    \"throughput_anomaly\",\n",
    "    \"availability_anomaly\",\n",
    "]\n",
    "titles = [\n",
    "    \"Latency Anomalies Over Time\",\n",
    "    \"Error Rate Anomalies Over Time\",\n",
    "    \"Throughput Anomalies Over Time\",\n",
    "    \"Availability Anomalies Over Time\",\n",
    "]\n",
    "colors = [\"blue\", \"green\", \"red\", \"purple\"]\n",
    "\n",
    "# Plot anomalies in a 2x2 grid layout\n",
    "plot_anomalies(data_with_anomalies, columns_to_plot, anomaly_columns, titles, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training\n",
    "\n",
    "In this section, we will select appropriate machine learning models and train them on the engineered features. This process involves choosing models that are well-suited to the problem, tuning their hyperparameters, and evaluating their performance.\n",
    "\n",
    "### Selecting Appropriate Machine Learning Models\n",
    "We will choose machine learning models that are suitable for predicting our target variables.\n",
    "\n",
    "### Splitting Data into Training and Testing Sets\n",
    "We will split the data into training and testing sets to evaluate model performance on unseen data.\n",
    "\n",
    "### Hyperparameter Tuning Using GridSearchCV\n",
    "To optimize model performance, we will perform hyperparameter tuning using GridSearchCV, which allows us to find the best combination of hyperparameters.\n",
    "\n",
    "### Evaluating Model Performance\n",
    "We will evaluate the trained models using appropriate metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and RÂ² Score.\n",
    "\n",
    "### Displaying Model Performance Metrics\n",
    "The results of the model evaluation will be displayed in a readable format to facilitate comparison and selection of the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with hyperparameter grids for tuning\n",
    "models_with_params = {\n",
    "    \"LinearRegression\": (LinearRegression(), {}),\n",
    "    \"RandomForestRegressor\": (\n",
    "        RandomForestRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [None, 10, 20, 30],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 4],\n",
    "        },\n",
    "    ),\n",
    "    \"GradientBoostingRegressor\": (\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "        },\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform hyperparameter tuning using GridSearchCV\n",
    "def hyperparameter_tuning(model, param_grid, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "    model (estimator): The machine learning model to tune.\n",
    "    param_grid (dict): The hyperparameter grid to search.\n",
    "    X_train (pd.DataFrame): Training feature data.\n",
    "    y_train (pd.Series): Training target data.\n",
    "\n",
    "    Returns:\n",
    "    estimator: The model with the best combination of hyperparameters.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning and evaluate models, display best model results\n",
    "\n",
    "# Define features and target\n",
    "features = data_fe.drop(columns=[\"timestamp\", \"latency\"])\n",
    "target = data_fe[\"latency\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning and evaluate models\n",
    "best_models = {}\n",
    "for model_name, (model, params) in models_with_params.items():\n",
    "    if params:\n",
    "        tuned_model = hyperparameter_tuning(model, params, X_train, y_train)\n",
    "    else:\n",
    "        tuned_model = model.fit(X_train, y_train)\n",
    "    best_models[model_name] = tuned_model\n",
    "    print(f\"Best model parameters for {model_name}: {tuned_model.get_params()}\")\n",
    "\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    return {\n",
    "        \"Model\": model.__class__.__name__,\n",
    "        \"Train MAE\": mean_absolute_error(y_train, y_pred_train),\n",
    "        \"Test MAE\": mean_absolute_error(y_test, y_pred_test),\n",
    "        \"Train RMSE\": mean_squared_error(y_train, y_pred_train, squared=False),\n",
    "        \"Test RMSE\": mean_squared_error(y_test, y_pred_test, squared=False),\n",
    "        \"Train R2\": r2_score(y_train, y_pred_train),\n",
    "        \"Test R2\": r2_score(y_test, y_pred_test),\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate each best model\n",
    "best_model_results = []\n",
    "for model_name, model in best_models.items():\n",
    "    results = evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "    best_model_results.append(results)\n",
    "\n",
    "# Create a DataFrame to display the best model results\n",
    "best_model_results_df = pd.DataFrame(best_model_results)\n",
    "\n",
    "# Display the results in a readable format\n",
    "print(\"\\nBest Model Performance Metrics (after tuning):\\n\")\n",
    "print(tabulate(best_model_results_df, headers=\"keys\", tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot model comparison with actual vs predicted and residuals\n",
    "def plot_model_comparison(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Plot comparison of actual vs predicted values and residuals for each model.\n",
    "\n",
    "    Parameters:\n",
    "    models (dict): Dictionary containing the trained models.\n",
    "    X_train (pd.DataFrame): Training feature data.\n",
    "    X_test (pd.DataFrame): Testing feature data.\n",
    "    y_train (pd.Series): Training target data.\n",
    "    y_test (pd.Series): Testing target data.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Comparison of actual vs predicted\n",
    "        plt.subplot(len(models), 2, i * 2 + 1)\n",
    "        plt.scatter(y_test, y_pred, alpha=0.3, color=\"blue\")\n",
    "        plt.plot(\n",
    "            [y_test.min(), y_test.max()],\n",
    "            [y_test.min(), y_test.max()],\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "        plt.xlabel(\"Actual Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(f\"Comparison of Actual vs Predicted Values for {model_name}\")\n",
    "\n",
    "        # Residuals plot\n",
    "        residuals = y_test - y_pred\n",
    "        plt.subplot(len(models), 2, i * 2 + 2)\n",
    "        plt.scatter(y_pred, residuals, alpha=0.3, color=\"green\")\n",
    "        plt.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "        plt.xlabel(\"Predicted Values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(f\"Residuals Plot for {model_name}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Plot the model comparison\n",
    "plot_model_comparison(best_models, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimal SLOs Using AI\n",
    "\n",
    "In this section, we will define the optimal Service Level Objectives (SLOs) based on our model predictions and analysis. We will present the final SLO definitions and explain why they were chosen over other possible options.\n",
    "\n",
    "### Analyzing Model Predictions\n",
    "We will use the predictions from the best models to understand the performance metrics.\n",
    "\n",
    "### Plot Predictions\n",
    "We will plot the model predictions to visualize the data distribution and identify trends.\n",
    "\n",
    "### Setting SLO Thresholds Based on Predicted Values\n",
    "We will establish thresholds for each metric based on the 95th percentile of the predicted values.\n",
    "\n",
    "### Justifying the Chosen Thresholds\n",
    "We will explain the rationale behind each threshold, considering factors such as industry standards, customer needs, and business objectives.\n",
    "\n",
    "### Visualizing and Comparing SLO Thresholds\n",
    "We will create visualizations to compare the preliminary SLO thresholds with those derived from AI predictions, highlighting the improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define and justify optimal SLOs\n",
    "def define_optimal_slos(model, data, target_column):\n",
    "    \"\"\"\n",
    "    Define and justify optimal SLOs using model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    model (estimator): The trained machine learning model.\n",
    "    data (pd.DataFrame): The dataframe containing the features.\n",
    "    target_column (str): The name of the target column for predictions.\n",
    "\n",
    "    Returns:\n",
    "    float: The optimal SLO threshold based on the 95th percentile of the predicted values.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(data.drop(columns=[\"timestamp\", target_column]))\n",
    "    data[\"predicted_\" + target_column] = predictions\n",
    "\n",
    "    slo_threshold = np.percentile(predictions, 95)\n",
    "    return slo_threshold, data\n",
    "\n",
    "\n",
    "# Train the best model on the entire dataset, make predictions, and define optimal SLOs\n",
    "best_model_name = best_model_results_df.loc[best_model_results_df[\"Test R2\"].idxmax()][\n",
    "    \"Model\"\n",
    "]\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "# Define optimal SLOs using AI\n",
    "slo_latency_ai, data_fe = define_optimal_slos(best_model, data_fe, \"latency\")\n",
    "\n",
    "print(f\"SLO for Latency based on AI (95th percentile): {slo_latency_ai} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot latency histogram with SLO thresholds and annotations\n",
    "def plot_latency_histogram_with_slo_comparison(\n",
    "    data, preliminary_slo, predicted_slo, column, color\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot latency histogram with SLO thresholds and annotations.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The dataframe containing the data.\n",
    "    preliminary_slo (float): The preliminary SLO threshold.\n",
    "    predicted_slo (float): The predicted SLO threshold based on AI.\n",
    "    column (str): The column name for latency data.\n",
    "    color (str): The color for the histogram plot.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.histplot(data[column], bins=30, kde=True, ax=ax, color=color)\n",
    "\n",
    "    # Plot preliminary SLO threshold\n",
    "    ax.axvline(\n",
    "        x=preliminary_slo,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Preliminary SLO: {preliminary_slo:.2f} ms\",\n",
    "    )\n",
    "\n",
    "    # Plot predicted SLO threshold\n",
    "    ax.axvline(\n",
    "        x=predicted_slo,\n",
    "        color=\"green\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Predicted SLO: {predicted_slo:.2f} ms\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Distribution of {column.capitalize()} with SLOs\", fontsize=16)\n",
    "    ax.set_xlabel(column.capitalize(), fontsize=14)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=14)\n",
    "    ax.legend(loc=\"upper right\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latency histogram with both preliminary and predicted SLO thresholds\n",
    "\n",
    "# Define column and color for the plot\n",
    "column_to_plot = \"latency\"\n",
    "color = \"blue\"\n",
    "\n",
    "print(\n",
    "    f\"Preliminary SLO for Latency (95th percentile): {slo_thresholds[column_to_plot]} ms\"\n",
    ")\n",
    "print(f\"Predicted SLO for Latency (95th percentile): {slo_latency_ai} ms\")\n",
    "\n",
    "# Plot latency histogram with both SLO thresholds\n",
    "plot_latency_histogram_with_slo_comparison(\n",
    "    data_fe, slo_thresholds[column_to_plot], slo_latency_ai, column_to_plot, color\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated the process of defining and optimizing Service Level Objectives (SLOs) using AI. We followed best practices in machine learning to ensure robust, reliable, and reproducible results. Here is a summary of the steps we took:\n",
    "\n",
    "### Summary of Steps\n",
    "1. **Generating Mock Data:**\n",
    "   - We generated realistic mock data to simulate real-world scenarios for SLO experimentation.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA):**\n",
    "   - We performed EDA to gain insights into the generated data, identify patterns, and detect outliers.\n",
    "\n",
    "3. **Defining SLOs Without AI:**\n",
    "   - We defined SLOs based on traditional methods and evaluated the data against these thresholds.\n",
    "\n",
    "4. **Feature Engineering:**\n",
    "   - We created new features and transformed existing ones to improve the predictive power of our machine learning models.\n",
    "\n",
    "5. **Anomaly Detection and Visualization:**\n",
    "   - We identified anomalies in the metrics and visualized them over time.\n",
    "\n",
    "6. **Model Selection and Training:**\n",
    "   - We selected appropriate machine learning models, performed hyperparameter tuning, and evaluated their performance.\n",
    "\n",
    "7. **Model Comparison:**\n",
    "   - We compared the performance of different models using actual vs predicted plots and residual plots.\n",
    "\n",
    "8. **Define Optimal SLOs Using AI:**\n",
    "   - We used the best-performing models to define optimal SLOs and provided a rationale for the chosen thresholds.\n",
    "\n",
    "### Importance of Following Best Practices\n",
    "By following best practices in data preparation, model training, and evaluation, we ensured that our machine learning models are reliable and reproducible. This approach helps in obtaining accurate and actionable insights from the data, leading to better decision-making and improved service quality.\n",
    "\n",
    "### Conclusion\n",
    "Using AI to define SLOs provided a more refined and accurate understanding of the service's performance metrics. The AI-based approach captured complex patterns and relationships in the data that traditional methods might miss, leading to more realistic and achievable SLOs. This improvement can help drive better performance optimization and resource management, ultimately leading to higher quality service and better alignment with business objectives.\n",
    "\n",
    "We hope this notebook serves as a useful guide for operations professionals looking to leverage AI for SLO optimization. Thank you for following along!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
